{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `validate_info` is not reachable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m workflow\u001b[38;5;241m.\u001b[39mset_entry_point(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m app \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m display(Image(workflow\u001b[38;5;241m.\u001b[39mget_graph(xray\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n",
      "File \u001b[1;32mh:\\github\\projects\\chatbot\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:430\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[1;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    427\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[0;32m    439\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m     ]\n\u001b[0;32m    448\u001b[0m )\n",
      "File \u001b[1;32mh:\\github\\projects\\chatbot\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:393\u001b[0m, in \u001b[0;36mGraph.validate\u001b[1;34m(self, interrupt)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not reachable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m END:\n",
      "\u001b[1;31mValueError\u001b[0m: Node `validate_info` is not reachable"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import re\n",
    "\n",
    "# Define our state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], list.append]\n",
    "    payee: str\n",
    "    amount: float\n",
    "    current_step: str\n",
    "\n",
    "# Initialize our LLM\n",
    "llm = ChatOllama(model=\"phi:3.5\", temperature=0)\n",
    "\n",
    "# Define our nodes\n",
    "def collect_info(state: State) -> State:\n",
    "    messages = state['messages']\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a banking assistant. Collect the payee name and payment amount from the user. Ask for one piece of information at a time.\"),\n",
    "        (\"human\", \"I want to make a payment.\"),\n",
    "        *messages\n",
    "    ])\n",
    "    response = llm.invoke(prompt.format())\n",
    "    state['messages'].append(AIMessage(content=response))\n",
    "    return state\n",
    "\n",
    "def validate_info(state: State) -> State:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-2].content if len(messages) >= 2 else \"\"\n",
    "    ai_response = messages[-1].content\n",
    "\n",
    "    # Check for payee\n",
    "    if state['payee'] == \"\" and \"name\" in last_message.lower():\n",
    "        payee_match = re.search(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', last_message)\n",
    "        if payee_match:\n",
    "            state['payee'] = payee_match.group()\n",
    "        else:\n",
    "            state['messages'].append(AIMessage(content=\"I'm sorry, I couldn't identify a valid name. Could you please provide the payee's full name (First Last)?\"))\n",
    "    \n",
    "    # Check for amount\n",
    "    if state['amount'] == 0 and \"amount\" in last_message.lower():\n",
    "        amount_match = re.search(r'\\$?(\\d+(\\.\\d{2})?)', last_message)\n",
    "        if amount_match:\n",
    "            state['amount'] = float(amount_match.group(1))\n",
    "        else:\n",
    "            state['messages'].append(AIMessage(content=\"I'm sorry, I couldn't identify a valid amount. Could you please provide the amount in dollars (e.g., 50.00)?\"))\n",
    "\n",
    "    # If both are collected, confirm\n",
    "    if state['payee'] != \"\" and state['amount'] != 0:\n",
    "        confirm_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a banking assistant. Confirm the payment details with the user.\"),\n",
    "            (\"human\", f\"Please confirm: Pay {state['payee']} the amount of ${state['amount']:.2f}. Is this correct?\")\n",
    "        ])\n",
    "        response = llm.invoke(confirm_prompt.format())\n",
    "        state['messages'].append(AIMessage(content=response))\n",
    "    \n",
    "    return state\n",
    "\n",
    "def process_payment(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a banking assistant. Process the payment and inform the user.\"),\n",
    "        (\"human\", f\"Process payment of ${state['amount']:.2f} to {state['payee']}.\"),\n",
    "    ])\n",
    "    response = llm.invoke(prompt.format())\n",
    "    state['messages'].append(AIMessage(content=response))\n",
    "    return state\n",
    "\n",
    "# Define our edges\n",
    "def should_continue_collecting(state: State):\n",
    "    if state['payee'] == \"\" or state['amount'] == 0:\n",
    "        return \"collect_info\"\n",
    "    else:\n",
    "        return \"validate_info\"\n",
    "\n",
    "def should_process_payment(state: State):\n",
    "    last_message = state['messages'][-2].content.lower()\n",
    "    if \"yes\" in last_message or \"correct\" in last_message:\n",
    "        return \"process_payment\"\n",
    "    elif \"no\" in last_message or \"incorrect\" in last_message:\n",
    "        state['payee'] = \"\"\n",
    "        state['amount'] = 0\n",
    "        return \"collect_info\"\n",
    "    else:\n",
    "        return \"validate_info\"\n",
    "\n",
    "def should_end(state: State):\n",
    "    return END\n",
    "\n",
    "# Create our graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"collect_info\", collect_info)\n",
    "workflow.add_node(\"validate_info\", validate_info)\n",
    "workflow.add_node(\"process_payment\", process_payment)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"collect_info\", should_continue_collecting)\n",
    "workflow.add_edge(\"validate_info\", should_continue_collecting)\n",
    "workflow.add_edge(\"validate_info\", should_process_payment)\n",
    "workflow.add_edge(\"process_payment\", should_end)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"collect_info\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "display(Image(workflow.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"payee\": \"\",\n",
    "    \"amount\": 0,\n",
    "    \"current_step\": \"collect_info\"\n",
    "}\n",
    "\n",
    "for output in app.stream(initial_state):\n",
    "    if '__end__' not in output:\n",
    "        response = output['messages'][-1].content\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        user_input = input(\"User: \")\n",
    "        output['messages'].append(HumanMessage(content=user_input))\n",
    "    else:\n",
    "        print(\"Transaction completed.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `process_payment` is not reachable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m workflow\u001b[38;5;241m.\u001b[39mset_entry_point(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m app \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     96\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayee\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfirmed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    101\u001b[0m }\n",
      "File \u001b[1;32mh:\\github\\projects\\chatbot\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:430\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[1;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    427\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[0;32m    439\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m     ]\n\u001b[0;32m    448\u001b[0m )\n",
      "File \u001b[1;32mh:\\github\\projects\\chatbot\\venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:393\u001b[0m, in \u001b[0;36mGraph.validate\u001b[1;34m(self, interrupt)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not reachable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m END:\n",
      "\u001b[1;31mValueError\u001b[0m: Node `process_payment` is not reachable"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import re\n",
    "\n",
    "# Define our state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], list.append]\n",
    "    payee: str\n",
    "    amount: float\n",
    "    confirmed: bool\n",
    "\n",
    "# Initialize our LLM\n",
    "llm = ChatOllama(model=\"phi:3.5\", temperature=0)\n",
    "\n",
    "# Define our nodes\n",
    "def collect_info(state: State) -> State:\n",
    "    messages = state['messages']\n",
    "    \n",
    "    if state['payee'] == \"\":\n",
    "        prompt = \"Please provide the payee's full name.\"\n",
    "    elif state['amount'] == 0:\n",
    "        prompt = f\"Please provide the amount to pay {state['payee']}.\"\n",
    "    else:\n",
    "        prompt = f\"Please confirm: Pay {state['payee']} the amount of ${state['amount']:.2f}. Is this correct? (Yes/No)\"\n",
    "\n",
    "    llm_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a banking assistant. Collect payment information from the user.\"),\n",
    "        (\"human\", prompt),\n",
    "        *messages\n",
    "    ])\n",
    "    response = llm.invoke(llm_prompt.format())\n",
    "    state['messages'].append(AIMessage(content=response))\n",
    "    return state\n",
    "\n",
    "def validate_info(state: State) -> State:\n",
    "    if len(state['messages']) > 0:\n",
    "        user_message = state['messages'][-2].content\n",
    "        if state['payee'] == \"\":\n",
    "            payee_match = re.search(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', user_message)\n",
    "            if payee_match:\n",
    "                state['payee'] = payee_match.group()\n",
    "        elif state['amount'] == 0:\n",
    "            amount_match = re.search(r'\\$?(\\d+(\\.\\d{2})?)', user_message)\n",
    "            if amount_match:\n",
    "                state['amount'] = float(amount_match.group(1))\n",
    "        elif not state['confirmed']:\n",
    "            if \"yes\" in user_message.lower():\n",
    "                state['confirmed'] = True\n",
    "            elif \"no\" in user_message.lower():\n",
    "                state['payee'] = \"\"\n",
    "                state['amount'] = 0\n",
    "    return state\n",
    "\n",
    "def process_payment(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a banking assistant. Process the payment and inform the user.\"),\n",
    "        (\"human\", f\"Process payment of ${state['amount']:.2f} to {state['payee']}.\"),\n",
    "    ])\n",
    "    response = llm.invoke(prompt.format())\n",
    "    state['messages'].append(AIMessage(content=response))\n",
    "    return state\n",
    "\n",
    "# Define our edges\n",
    "def router(state: State):\n",
    "    if not state['confirmed']:\n",
    "        if state['payee'] == \"\" or state['amount'] == 0:\n",
    "            return \"collect_info\"\n",
    "        else:\n",
    "            return \"validate_info\"\n",
    "    else:\n",
    "        return \"process_payment\"\n",
    "\n",
    "# Create our graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"collect_info\", collect_info)\n",
    "workflow.add_node(\"validate_info\", validate_info)\n",
    "workflow.add_node(\"process_payment\", process_payment)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"collect_info\", \"validate_info\")\n",
    "workflow.add_edge(\"validate_info\", router)\n",
    "workflow.add_edge(\"process_payment\", END)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"collect_info\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"payee\": \"\",\n",
    "    \"amount\": 0,\n",
    "    \"confirmed\": False\n",
    "}\n",
    "\n",
    "for output in app.stream(initial_state):\n",
    "    if '__end__' not in output:\n",
    "        response = output['messages'][-1].content\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        user_input = input(\"User: \")\n",
    "        output['messages'].append(HumanMessage(content=user_input))\n",
    "    else:\n",
    "        print(\"Transaction completed.\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
